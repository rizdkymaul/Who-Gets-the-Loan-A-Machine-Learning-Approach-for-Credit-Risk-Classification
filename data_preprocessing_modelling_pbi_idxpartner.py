# -*- coding: utf-8 -*-
"""Data PreProcessing-Modelling_PBI-IDXPartner.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1olfwAtVgr-ZsooDyJpv5QAV2s_jMayFu

# **Prediction Model for Assessing Client Credit Eligibility**

# **FINAL TASK PROJECT BASED INTERNSHIP : Data Scientist - id/x Partners x Rakamin Academy**

## **Business Understanding**

**- Company Backround**

ID/X Partners adalah perusahaan konsultan yang telah beroperasi sejak tahun 2002, berfokus pada solusi Data Analytics and Decisioning (DAD) untuk manajemen risiko dan strategi pemasaran. Dalam setiap proyeknya, ID/X mengedepankan nilai inti CHAMPION, termasuk pengambilan keputusan berbasis data (Numeric), ketangkasan (Agility), dan inovasi (Innovative).

**- Project Overview**

Proyek ini bertujuan membangun model prediktif untuk mengklasifikasikan calon peminjam ke dalam dua kategori utama: High-Risk Loan dan Low-Risk Loan. Dengan pendekatan data-driven, model ini membantu perusahaan lending membuat keputusan kredit yang lebih akurat dan meminimalkan risiko gagal bayar.

**- Problem Statement**

Tingginya tingkat gagal bayar menjadi tantangan utama bagi perusahaan lending dalam menilai kelayakan kredit calon peminjam.
Kesalahan penilaian berdampak langsung pada kerugian finansial dan efisiensi operasional.

Melalui proyek "Prediction Model for Assessing Client Credit Eligibility", saya membangun solusi end-to-end berbasis data science untuk memprediksi risiko kredit sebelum pinjaman disetujui, Menyimulasikan dampak bisnis dari model prediktif, Menyampaikan hasil dalam bentuk visual yang informatif dan strategis.

**- Goals**

Membangun model prediksi risiko kredit guna mengurangi tingkat gagal bayar dan meningkatkan profitabilitas perusahaan lending.

**- Objective**

1. Mengklasifikasikan peminjam ke dalam kategori High-Risk Loan dan Low-Risk Loan.

2. Meningkatkan akurasi penilaian risiko sebelum pinjaman disetujui.

3. Mengukur dampak bisnis dari implementasi model prediktif terhadap revenue dan risiko.


**- Business Metrics**

1. High-Risk Rate – Default Rate (DR)

2. Total High-Risk Loans – Exposure at Default (EAD)

3. Total Revenue – Expected Interest Income (EII)

4. Total High-Risk Loss – Expected Loss (EL)

5. Net Revenue – Net Interest Income (NII)

[**Dataset Dictionary**](https://docs.google.com/spreadsheets/d/1iT1JNOBwU4l616_rnJpo0iny7blZvNBs/edit?gid=1666154857#gid=1666154857)

## **Import Libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency, f_oneway
from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from imblearn.over_sampling import SMOTE

import re
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')


file_path = '/content/drive/My Drive/loan_data_2007_2014.csv'
df = pd.read_csv(file_path, low_memory=True)  # Meminimalkan penggunaan memori
df_simulation = df.copy()
df.head()

# split data by rows and columns
rows = df.shape[0]
cols = df.shape[1]

# display data shape
print("Data shape:")
print(f"Number of rows: {rows} \nNumber of columns: {cols}")

df.info()

"""Analisis dataset menunjukkan bahwa terdapat 74 kolom dengan berbagai tipe data, termasuk float, integer, dan object, serta total 466.285 baris. Namun, beberapa kolom mengalami missing values, dan beberapa di antaranya sepenuhnya kosong. Temuan ini menekankan pentingnya pemrosesan data yang cermat untuk menjaga kualitas analisis.

## **Invalid Values**
"""

# Perbaiki fungsi untuk konversi full year
def convert_to_full_year_with_month(data_str):
    if pd.isnull(data_str):
        return None
    month, year = data_str.split('-')
    year = int(year)
    if year >= 40:  # Asumsikan data sebelum tahun 1940 tidak ada
        year_str = f'19{year}'
    else:
        year_str = f'20{year:02d}'
    return f'{month}-{year_str}'

# Terapkan fungsi konversi ke setiap kolom
date_columns = ['issue_d', 'last_pymnt_d', 'earliest_cr_line', 'last_credit_pull_d']
for col in date_columns:
    df[col] = df[col].apply(convert_to_full_year_with_month)
    df[col] = pd.to_datetime(df[col], format='%b-%Y')

"""# **Data Pre-processing**

### **Data Cleansing**

#### **Handling Missing value**

#### **1. Empty Column**
"""

# Calculate null values
null_values = df.isnull().sum()

# Calculate total null values and percentage
nvc = pd.DataFrame(null_values.sort_values(), columns=['Total Null Values'])
nvc['Percentage'] = (nvc['Total Null Values'] / df.shape[0]) * 100
nvc["Data Type"] = [df[col].dtype for col in df.columns]
nvc.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)

# Display the results
nvc.style.background_gradient(cmap='Blues')

# display all the empty columns (no data)
mis_val = df.isna().sum()
empty_col = mis_val[mis_val == df.shape[0]]
empty_col

"""Terdapat 17 kolom bernilai kosong"""

empty_col_names = empty_col.index # get all the empty column names
df = df.drop(columns=empty_col_names) # drop all empty columns

"""#### **2. Column Non-empty**"""

# Fungsi untuk menghitung dan menampilkan kolom dengan nilai missing
def check_nan(df):
    # Hitung nilai null
    null_values = df.isnull().sum()

    # Buat DataFrame untuk total nilai null dan persentase
    nvc = pd.DataFrame(null_values[null_values > 0].sort_values(), columns=['Total Null Values'])  # Hanya ambil kolom dengan nilai null
    nvc['Percentage'] = (nvc['Total Null Values'] / df.shape[0]) * 100
    nvc["Data Type"] = [df[col].dtype for col in nvc.index]  # Ambil tipe data dari kolom yang ada

    # Urutkan berdasarkan total nilai null dan persentase
    nvc.sort_values(by=["Total Null Values", "Percentage"], ascending=False, inplace=True)

    # Tampilkan hasil dengan gaya
    return nvc.style.background_gradient(cmap='Blues')

# Panggil fungsi dan simpan hasilnya
styled_result = check_nan(df)

# Untuk menampilkan hasil di Jupyter Notebook, gunakan:
styled_result

"""Kolom-kolom seperti title, desc, emp_title, dan emp_length yang memiliki nilai null dapat diisi dengan nilai seperti 'Unknown' atau 'Not Recorded', mengingat sifatnya yang sangat terpersonalisasi dan bergantung pada peminjam.

ada beberapa kolom yang memiliki jumlah null value nya sama
- `total_rev_hi_lim`, `tot_cur_bal`, dan `tot_coll_amnt` **70.276 baris**
- `delinq_2yrs`, `pub_rec`, `inq_last_6mths`, `open_acc`, `total_acc`, `earliest_cr_line`, dan `acc_now_delinq` **29 baris**
"""

# Pilih kolom yang memiliki 29 nilai missing
filtered_cols = df[['delinq_2yrs', 'pub_rec', 'inq_last_6mths', 'open_acc', 'total_acc', 'earliest_cr_line', 'acc_now_delinq']]

# Periksa apakah nilai null di setiap kolom berada di baris yang sama
missing_rows = filtered_cols[filtered_cols.isna().all(axis=1)]
display(missing_rows)

# Cetak total baris dengan nilai missing
print('\nTotal baris:', len(missing_rows))

"""Missing value terdapat dalam baris yang sama, sehingga baris kosong ini akan dihapus."""

# Hapus baris yang memiliki nilai missing dari kolom yang telah difilter di atas
df = df.dropna(subset=filtered_cols.columns)

# Periksa kembali informasi nilai missing
check_nan(df)

filter_cols_mths = df[['mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_last_delinq']]
filter_cols_mths.describe()

"""Semua kolom"""

# Mengisi nilai missing dengan 0

## 1. Mengisi nilai missing di kolom mths_since_last_delinq untuk delinq_2yrs = 0
df.loc[(df['mths_since_last_delinq'].isna()) & (df['delinq_2yrs'] == 0.0), 'mths_since_last_delinq'] = 0

## 2. Mengisi nilai missing di kolom mths_since_last_record untuk pub_rec = 0
df.loc[(df['mths_since_last_record'].isna()) & (df['pub_rec'] == 0.0), 'mths_since_last_record'] = 0

## 3. Mengisi nilai missing di kolom mths_since_last_major_derog untuk pub_rec = 0 dan delinq_2yrs = 0
df.loc[(df['mths_since_last_major_derog'].isna()) & (df['pub_rec'] == 0.0) & (df['delinq_2yrs'] == 0.0), 'mths_since_last_major_derog'] = 0

"""berdasarkan asumsi logis:
Kalau seseorang tidak pernah mengalami suatu kejadian (misal menunggak, punya record, atau derogatori), maka wajar jika kolom tentang "berapa bulan sejak kejadian" itu kosong — dan itu bisa diisi dengan 0 (alias tidak pernah terjadi).

Ini adalah contoh rule-based imputation yang sangat berguna agar data tetap bersih dan tidak bias dalam modeling.
"""

# Mengisi nilai missing dengan -1 untuk data yang tidak diketahui
df[['mths_since_last_delinq', 'mths_since_last_major_derog']] = df[['mths_since_last_delinq', 'mths_since_last_major_derog']].fillna(-1)

"""Setelah melakukan imputasi 0 secara selektif sebelumnya (berdasarkan logika bahwa nasabah tidak pernah mengalami peristiwa tertentu), sisa missing values (NaN) yang tidak bisa dipastikan statusnya akan diisi dengan -1. Ini dilakukan untuk:

Nilai -1 bukan nilai aktual, tapi penanda bahwa sistem tidak tahu kapan terakhir kali kejadian itu terjadi.

Berbeda dari 0 yang berarti “tidak pernah terjadi”, nilai -1 ini menunjukkan bahwa kemungkinan pernah terjadi, tapi datanya tidak tersedia.
"""

# recheck missing values
check_nan(df)

# Periksa baris yang memiliki nilai null di kolom last_pymnt_d dengan informasi dari kolom lain
df.loc[df['last_pymnt_d'].isna(), ['last_pymnt_d', 'loan_status', 'last_credit_pull_d', 'last_pymnt_amnt', 'total_pymnt']]

# Periksa nilai unik dari loan_status di mana next_pymnt_d adalah null
df.loc[(df['next_pymnt_d'].isna()), 'loan_status'].value_counts()

# Check unique values for specified columns
cols_to_check = ['last_pymnt_d', 'loan_status', 'last_credit_pull_d', 'last_pymnt_amnt', 'total_pymnt']
for col in cols_to_check:
    print(f"Unique values for column '{col}':")
    print(df[col].unique())
    print("-" * 20)

# Check unique values for 'next_pymnt_d'
print(df['next_pymnt_d'].unique())

def replace_with_value(df, col_list):
    for col in col_list:
        # convert datetime to string
        if df[col].dtype == 'datetime64[ns]':
            df[col] = df[col].astype(str).replace('NaT', 'Not Recorded')

        # fill NaNs with 'Not Recorded'
        df[col] = df[col].fillna('Not Recorded')

        # special case: replace 'Not Recorded' with 'Loan Closed'
        if col == 'next_pymnt_d':
            df[col] = df[col].replace('Not Recorded', 'Loan Closed')

    return df

# function call
cat_cols_to_impute = ['title', 'desc', 'emp_title', 'emp_length', 'last_pymnt_d', 'last_credit_pull_d', 'next_pymnt_d']
replace_with_value(df, cat_cols_to_impute)

# recheck null values
check_nan(df)

# Check unique values for 'next_pymnt_d'
print(df['next_pymnt_d'].unique())

"""1. Datetime → string → isi "Not Recorded"

*  Konversi diperlukan karena NaT (Not a Time) pada datetime harus diubah menjadi string sebelum bisa diganti.

*  "Not Recorded" digunakan sebagai penanda eksplisit bahwa data tidak tersedia, tapi tidak berarti kosong/null dalam konteks modeling atau visualisasi.

2. Handling khusus untuk next_pymnt_d

*  Logika: kalau tidak ada tanggal pembayaran berikutnya, kemungkinan besar berarti pinjaman sudah lunas.

*  Jadi "Loan Closed" lebih informatif dan menggambarkan status aktual — bukan sekadar data kosong.

3. Untuk kolom kategorikal lain

* Contoh kolom seperti title, desc, emp_title, dan emp_length, jika kosong, dianggap tidak tercatat, jadi diisi dengan "Not Recorded".
"""

# drop the missing values
df = df.dropna(subset='collections_12_mths_ex_med')

"""tidak memiliki arti atau tidak bisa diimputasi dengan logika yang kuat, sehingga lebih aman dibuang saja."""

# check the correlation
df[['tot_coll_amt', 'collections_12_mths_ex_med', 'recoveries', 'collection_recovery_fee']].corr()

# check the correlation
df[['tot_coll_amt', 'collections_12_mths_ex_med', 'recoveries', 'collection_recovery_fee']].describe()

# impute with 0:
df['tot_coll_amt'] = df['tot_coll_amt'].fillna(0)

"""Kolom tot_coll_amt (Total Collection Amount) menunjukkan jumlah total tagihan yang masuk koleksi (penagihan pihak ketiga).

Jadi, ketika nilainya kosong (NaN), besar kemungkinan artinya adalah:

Nasabah tidak memiliki koleksi/tagihan yang masuk koleksi.

Sehingga secara logis dan aman, NaN dianggap = 0 (tidak punya utang koleksi).
"""

cols_to_describe = ['tot_cur_bal', 'total_pymnt', 'loan_amnt', 'revol_bal', 'out_prncp', 'installment', 'total_rec_int', 'total_rec_prncp', 'open_acc', 'total_acc']
description = df[cols_to_describe].describe()
description

# impute with median
cur_bal_median = df['tot_cur_bal'].median()
df['tot_cur_bal'] = df['tot_cur_bal'].fillna(cur_bal_median)

"""Kolom tot_cur_bal = Total Current Balance (jumlah saldo total saat ini dari semua akun).

Menggunakan median (nilai tengah) lebih aman dibandingkan mean karena tidak terpengaruh outlier.
"""

df['rev_lim_bal_ratio'] = df['total_rev_hi_lim'] / df['revol_bal']
median_ratio = df['rev_lim_bal_ratio'].median()

# impute total_rev_hi_lim with the median ratio
df.loc[df['total_rev_hi_lim'].isna(), 'total_rev_hi_lim'] = df['revol_bal'] * median_ratio

df = df.drop('rev_lim_bal_ratio', axis=1)

"""Menggunakan data relasional antar kolom, membuat hasilnya lebih masuk akal."""

# recheck missing values
check_nan(df)

# check the correlation between these columns
df[['revol_util', 'revol_bal', 'dti', 'int_rate', 'total_rec_int']].corr()

# check the null rows in revol_util where the value of revol_bal = 0
df.loc[(df['revol_util'].isna()) & (df['revol_bal'] == 0), ['revol_util', 'revol_bal', 'total_rev_hi_lim']]

rev_util_formula = (df['revol_bal'] / df['total_rev_hi_lim']) * 100 # revol util formula

# impute using formula if both revol_bal and total_rev_hi_lim > 0
df.loc[df['revol_util'].isna(), 'revol_util'] = rev_util_formula

# impute the rest with 0
df['revol_util'] = df['revol_util'].fillna(0)

"""Revolving Utilization = persentase penggunaan batas kredit (mirip seperti credit card utilization).

Karena kolom revol_util berisi persentase penggunaan kredit, maka nilai kosong (missing) sebaiknya diisi dengan logika yang masuk akal,
"""

# recheck null values
check_nan(df)

"""#### **Handling Duplicated Data**"""

# Mengecek apakah ada duplikat di setiap baris
duplicates = df.duplicated()

# Menampilkan jumlah duplikat dan baris yang duplikat
print(f"Jumlah baris duplikat: {duplicates.sum()}")
print("Baris yang duplikat:")
print(df[duplicates])

"""## **Handling Outlier**"""

print(df['policy_code'])

df = df.drop(columns=['Unnamed: 0', 'id', 'member_id','policy_code'], errors='ignore')

# Get numerical columns from the DataFrame 'df'
num_columns = df.select_dtypes(include=['number']).columns

# Now you can use 'num_columns' as intended
num_columns # numerical columns

import matplotlib.pyplot as plt
import seaborn as sns
import math

def plot_outliers(df, num_cols, cols_per_row=4, row_height=4, col_width=5):
    total = len(num_cols)
    rows = math.ceil(total / cols_per_row)
    figsize = (col_width * cols_per_row, row_height * rows)

    plt.style.use('default')  # Background putih
    fig, axes = plt.subplots(rows, cols_per_row, figsize=figsize)
    axes = axes.flatten()

    for i, col in enumerate(num_cols):
        sns.boxplot(y=df[col], ax=axes[i], color='steelblue', linewidth=1)
        axes[i].set_title(f'{col}', fontsize=10)
        axes[i].set_ylabel('')
        axes[i].grid(True, linestyle='--', alpha=0.5)

    # Hapus subplot kosong
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.suptitle('Boxplots of Numerical Features', fontsize=14, y=1.02)
    plt.show()

# function call
plot_outliers(df, num_columns)

"""Untuk outlier akan di lakukan metode Capping oleh karena itu kita butuh summary outlier tahu batas bawah/atas dan bersiap untuk trimming atau capping IQR"""

def outliers_info(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    upper_bound = Q3 + 1.5 * IQR
    lower_bound = Q1 - 1.5 * IQR

    count = ((series < lower_bound) | (series > upper_bound)).sum()
    return count, upper_bound, lower_bound

def print_outliers(df, num_cols):
    outliers_dict = {}  # store the column name and the number of outliers present

    for col in num_cols:
        count, upper, lower = outliers_info(df[col])  # gunakan variabel baru

        if count > 0:
            outliers_dict[col] = count  # simpan hanya kolom yang punya outlier

    # Sort hasilnya dari yang paling banyak outlier
    sorted_outliers = dict(sorted(outliers_dict.items(), key=lambda item: item[1], reverse=True))

    # Tampilkan
    for col, val in sorted_outliers.items():
        print(f'Total outliers in {col}: {val} ({((val/len(df)) * 100):.2f}%)')

    print(f'\nTotal columns with outliers: {len(sorted_outliers)} out of {len(num_cols)}')

print_outliers(df, num_columns)

"""Dengan 29 dari 32 kolom mengandung outliers, dan beberapa lebih dari 30% nilainya outlier, capping dengan IQR adalah pendekatan paling aman, efisien, dan efektif untuk menjaga kualitas data tanpa kehilangan banyak informasi.

Kalau langsung di-drop, akan kehilangan sebagian besar data, dan itu bisa bikin model jadi underfit atau tidak representatif.
"""

def cap_outliers(df, num_cols):
  for col in num_cols:
    _, upper_bound, lower_bound = outliers_info(df[col])  # Ambil batas atas & bawah via IQR
    df[col] = df[col].clip(lower_bound, upper_bound)      # Potong (clip) nilai ekstrem
  return df

# function call
df_capped = cap_outliers(df, num_columns)

# recheck outliers
print_outliers(df_capped, num_columns)

"""data sekarang telah bebas dari outlier setelah proses capping, dan siap digunakan untuk tahap berikutnya"""

# #boxplot after capping
# plot_outliers(df_capped, num_columns)

"""##**Labelling**

"""

loan_status_counts = df_capped['loan_status'].value_counts()
loan_status_counts

"""Penjelasan mengenai nilai dari loan_status :

* Fully Paid = Pinjaman telah dilunasi sepenuhnya oleh peminjam.

* Charged Off = Pinjaman dianggap tidak tertagih karena gagal bayar dalam jangka waktu lama.

* Current = Pinjaman masih berjalan, dan peminjam melakukan pembayaran sesuai jadwal.

* Default = Pinjaman telah melewati batas keterlambatan tertentu dan dianggap gagal bayar permanen.

* Late (31-120 days) = Pembayaran telah terlambat antara 31 hingga 120 hari.

* In Grace Period = Peminjam melewati tanggal jatuh tempo tetapi masih dalam masa tenggang sebelum dianggap terlambat.

* Late (16-30 days) = Pembayaran tertunda antara 16 hingga 30 hari.

* Does not meet the credit policy. Status: Fully Paid = Tidak memenuhi kebijakan kredit tetapi berhasil melunasi pinjaman.

* Does not meet the credit policy. Status: Charged Off = Tidak memenuhi kebijakan kredit dan akhirnya gagal bayar.

Dalam analisis ini, fitur loan_status dipilih sebagai label target karena memiliki nilai yang jelas dan informatif, seperti Fully Paid, Current, Default, Charged Off, dan lainnya. Untuk memahami pola kredit dengan lebih baik, status pinjaman ini dikelompokkan ke dalam dua kategori utama:

> **Low-Risk Loan**: Mencakup pinjaman yang menunjukkan rekam jejak pembayaran yang baik, seperti **Fully Paid, Current, dan Does not meet the credit policy (Fully Paid)**.

> **High-Risk Loan**: Mencakup pinjaman yang memiliki risiko gagal bayar tinggi, seperti **Default, Charged Off, Late (31-120 days), Late (16-30 days), Does not meet the credit policy (Charged Off), dan In Grace Period**.

Sehingga dalam proses selanjutnya dilakukan visualisasi distribusi status pinjaman dan menganalisis total jumlah pinjaman yang diterima berdasarkan setiap kategori. Hal ini akan membantu dalam mengidentifikasi pola pembayaran dan memahami kategori mana yang mendominasi dalam jumlah pinjaman yang diberikan.
"""

# Mapping status as high-risk or low-risk
def map_loan_status(status):
    """
    Fungsi untuk mengelompokkan status pinjaman menjadi 'high-risk' atau 'low-risk'.
    """
    default_statuses = [
        'Charged Off',
        'Late (31-120 days)',
        'Late (16-30 days)',
        'In Grace Period',
        'Default',
        'Does not meet the credit policy. Status:Charged Off'
    ]
    return 'high-risk' if status in default_statuses else 'low-risk'

# Apply mapping: hasilnya akan jadi string 'high-risk' atau 'low-risk'
df_capped['loan_status'] = df_capped['loan_status'].apply(map_loan_status)

# Encode hasil mapping langsung ke angka: 'high-risk' = 1, 'low-risk' = 0
df_capped['loan_status'] = df_capped['loan_status'].map({'low-risk': 0, 'high-risk': 1})
print(df_capped['loan_status'].head(10))

"""## **Feature Engineering**

### **Create New Feature**
"""

# Konversi semua kolom tanggal ke datetime (jika belum)
date_cols = ['issue_d', 'last_pymnt_d', 'last_credit_pull_d', 'earliest_cr_line']
for col in date_cols:
    if col in df_capped.columns:
        df_capped[col] = pd.to_datetime(df_capped[col], errors='coerce')

# Ekstrak informasi waktu dari issue_d
df_capped['issue_year'] = df_capped['issue_d'].dt.year
df_capped['issue_month'] = df_capped['issue_d'].dt.month

# Menghitung payment_time dalam bulan
df_capped['payment_time'] = (df_capped['last_pymnt_d'].dt.year - df_capped['issue_d'].dt.year) * 12 + \
                            (df_capped['last_pymnt_d'].dt.month - df_capped['issue_d'].dt.month)

# Menghitung loan_time dalam bulan
df_capped['loan_time'] = (df_capped['last_credit_pull_d'].dt.year - df_capped['earliest_cr_line'].dt.year) * 12 + \
                         (df_capped['last_credit_pull_d'].dt.month - df_capped['earliest_cr_line'].dt.month)

# Hapus kolom yang menyebabkan data leakage
df_capped.drop(columns=['earliest_cr_line', 'issue_d', 'last_pymnt_d', 'last_credit_pull_d'], inplace=True)

"""1. Ekstraksi Informasi Waktu dari issue_d

Bisa digunakan untuk analisis tren pinjaman berdasarkan tahun dan musim (misalnya: apakah lebih banyak pinjaman diajukan di awal tahun atau akhir tahun?).

2. Menghitung payment_time dalam Bulan

Bisa digunakan untuk memahami seberapa lama seseorang membayar pinjamannya.

3. Menghitung loan_time dalam Bulan

Bisa digunakan untuk memahami seberapa aktif peminjam dalam sistem kredit.
"""

# Recheck null values
check_nan(df_capped)

# Assuming 'df' is your DataFrame (as defined in the provided code)
df_capped[['payment_time', 'loan_time']].describe()

# Pilih kolom numerik yang ingin dihitung korelasinya
corr_cols = ['payment_time', 'loan_time', 'loan_status']
corr_matrix = df_capped[corr_cols].corr()

# Visualisasi heatmap korelasi
plt.figure(figsize=(6, 4))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Korelasi antar Fitur dan Loan Status")
plt.show()

# Nilai korelasi langsung terhadap target
print(corr_matrix['loan_status'])

"""* payment_time masih cukup layak disimpan, karena korelasi mendekati -0.2. Bisa diuji nanti di modeling.

* loan_time punya korelasi lemah banget,
drop dari awal untuk menjaga fitur tetap ringkas


"""

# Isi NaN pada payment_time dengan median
df_capped['payment_time'].fillna(df_capped['payment_time'].median(), inplace=True)

# Hapus kolom loan_time karena korelasinya rendah dan dianggap kurang relevan
df_capped.drop('loan_time', axis=1, inplace=True)

# Recheck null values
check_nan(df_capped)

"""####**Buang Kolom Redundan & High Cardinality**"""

# Memilih kolom dengan tipe data numerik
numeric_columns = df_capped.select_dtypes(include=['number']).columns
numeric_columns

df_capped.select_dtypes('object').columns

# Daftar kolom kategoric dengan Redundan dan high cardinality
high_card_cols = ['emp_title', 'url', 'desc', 'title',
                  'zip_code', 'next_pymnt_d','application_type']
df_capped = df_capped.drop(columns=high_card_cols)

print(df_capped.columns)

"""Beberapa kolom seperti url, desc, title, emp_title, dan zip_code memiliki terlalu banyak nilai unik (high cardinality) atau tidak memberikan informasi yang berguna untuk prediksi. Kolom seperti id, member_id, dan Unnamed: 0 juga tidak punya nilai prediktif dan hanya menjadi noise. Maka dari itu, kolom-kolom ini dihapus agar model lebih efisien dan akurat."""

df_capped.select_dtypes('object').columns

"""#### **Create state_region based addr_state**"""

# Definisikan mapping
regions = {
    'West':     ['CA', 'NV', 'UT', 'CO', 'WY', 'MT', 'ID', 'OR', 'WA', 'AK', 'HI'],
    'Southwest':['AZ', 'NM', 'TX', 'OK'],
    'Midwest':  ['ND', 'MN', 'SD', 'NE', 'KS', 'MO', 'IA', 'WI', 'IL', 'IN', 'OH', 'MI'],
    'Southeast':['AR', 'LA', 'MS', 'AL', 'GA', 'TN', 'KY', 'NC', 'SC', 'WV', 'DC', 'VA', 'DE', 'FL'],
    'Northeast':['PA', 'MD', 'NJ', 'NY', 'CT', 'RI', 'MA', 'NH', 'VT', 'ME']
}

# Inisialisasi kolom baru
df_capped['state_region'] = 'Unknown'

# Looping dan assign nilai
for region, states in regions.items():
    df_capped.loc[df_capped['addr_state'].isin(states), 'state_region'] = region

df_capped.drop(columns=['addr_state'], inplace=True)

# tampilkan hasil
display(df_capped[['state_region']])

"""Menyederhanakan visualisasi (dari 50+ state jadi hanya 5 region)

#### **Create grouped_purpose based purpose**
"""

# definisikan fungsi custom
def group_purpose(purpose):
    if purpose in ['credit_card', 'debt_consolidation']:
        return 'Debt Management'
    elif purpose in ['small_business', 'educational']:
        return 'Business & Education'
    elif purpose in ['home_improvement', 'house', 'renewable_energy', 'major_purchase', 'car']:
        return 'Home & Major Purchases'
    elif purpose in ['medical', 'moving', 'vacation', 'wedding']:
        return 'Personal & Lifestyle'
    elif purpose == 'other':
        return 'Other'
    else:
        return 'Unknown'

# apply ke kolom
df_capped['grouped_purpose'] = df_capped['purpose'].apply(group_purpose)

# tampilkan hasil
display(df_capped[['purpose', 'grouped_purpose']])

# hapus kolom asal jika perlu
df_capped = df_capped.drop(columns=['purpose'])

"""Mengelompokkan nilai pada kolom purpose (yang banyak dan spesifik) ke dalam kategori yang lebih umum, Menghindari kategori yang terlalu granular.

#### **Create Spesific Term**
"""

# remove the word 'months' in term and convert it to numerical type
df_capped['term'] = df_capped['term'].str.replace(' months', '').astype(int)

"""## **Feature Encoding**

### **Manual Encoding emp_lenght**
"""

# Membuat dictionary untuk mengubah durasi kerja menjadi angka
emp_length_mapping = {
    '< 1 year': 0,         # Kurang dari 1 tahun dianggap 0
    '1 year': 1,           # 1 tahun jadi 1
    '2 years': 2,
    '3 years': 3,
    '4 years': 4,
    '5 years': 5,
    '6 years': 6,
    '7 years': 7,
    '8 years': 8,
    '9 years': 9,
    '10+ years': 10,       # Lebih dari 10 tahun jadi 10
    'Not Recorded': -1     # Data tidak tersedia diisi -1 sebagai penanda
}

# Menerapkan mapping ke kolom emp_length
df_capped['emp_length'] = df_capped['emp_length'].map(emp_length_mapping)

display(df_capped['emp_length'])

"""Mapping emp_length ke angka seperti ini sangat disarankan untuk pemrosesan data kategorikal ordinal.

Daripada membiarkan missing values kosong (yang bisa bikin error di model), kita isi dengan -1 agar nanti bisa diperlakukan khusus (misal: diimputasi atau dimasukkan ke kategori "unknown").

Menjaga makna asli data (semakin lama kerja = semakin tinggi nilai),
"""

# binary encoding for column pymnt_plan and initial_list_status
df_capped['pymnt_plan'] = df_capped['pymnt_plan'].map({'n': 0, 'y': 1})
df_capped['initial_list_status'] = df_capped['initial_list_status'].map({'f': 0, 'w':1})

"""### **Manual Encoding grade**

"""

# Encoding ordinal untuk grade (semakin besar, semakin high-risk)
df_capped['grade'] = df_capped['grade'].map(lambda x: ord(x) - ord('A') + 1)

# Encoding sub_grade secara otomatis berdasarkan urutan (ascending)
sub_grade_mapping = {grade: idx for idx, grade in enumerate(sorted(df_capped['sub_grade'].unique()), start=1)}
df_capped['sub_grade'] = df_capped['sub_grade'].map(sub_grade_mapping)

# Tampilkan hasil
df_capped[['grade', 'sub_grade']]

"""Karena grade dan sub_grade punya urutan makna (misalnya: A lebih baik dari B, B lebih baik dari C), maka  digunakan ordinal encoding agar urutan tersebut tidak hilang.


"""

df_capped

"""### **One Hot Encoding or Target Encoding**"""

cat_columns = df_capped.select_dtypes('object').columns
cat_columns

# # one hot encoding using pd.get_dummies()
# df_capped = pd.get_dummies(df_capped, columns=cat_columns, drop_first=True)

# # df info after encoding
# df_capped.info()

# # display result
# display(df_capped.head(15))

"""Kolom-kolom kategorikal yang tersisa diolah dengan one-hot encoding menggunakan pd.get_dummies(). Karena nilainya sudah terkelompok, hasil encoding tetap efisien tanpa membanjiri dataset dengan kolom baru. Setelah proses ini, total kolom menjadi 55.

**Split data X & y for target Encoding**
"""

from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif
import pandas as pd

X = df_capped.drop(columns='loan_status')
y = df_capped['loan_status']

cat_columns = X.select_dtypes('object').columns
cat_columns

!pip install category_encoders

from category_encoders import TargetEncoder

# Tentukan kolom kategorikal
target_encode_cols = X.select_dtypes('object').columns

# Inisialisasi dan fit-transform encoder
target_encoder = TargetEncoder(cols=target_encode_cols)
X_encod = X.copy()
X_encod[target_encode_cols] = target_encoder.fit_transform(X[target_encode_cols], y)

# Sebelum encode (dari X)
print("Sebelum Encoding:")
print(X[target_encode_cols].head())

# Sesudah encode (dari X_encod)
print("\nSetelah Encoding:")
print(X_encod[target_encode_cols].head())

"""## **Feature Selection**

### **Correlation Matrix of Numerical Variables**
"""

# Ambil kolom numerik dari X_filtered
numeric_cols = X_encod.select_dtypes(include=['number'])

# Hitung korelasi
num_corr = numeric_cols.corr()

# Plot heatmap korelasi
plt.figure(figsize=(15, 11))
sns.heatmap(num_corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix of Numerical Variables - X_filtered', fontsize=16, fontweight='bold')
plt.show()

# Filter hanya korelasi yang signifikan > 0.85 (selain diagonal 1.0)
filtered_corr = num_corr.where((num_corr > 0.85) & (num_corr < 1.0))

# Plot heatmap korelasi yang difilter
plt.figure(figsize=(15, 11))
sns.heatmap(filtered_corr, annot=True, cmap='coolwarm', mask=filtered_corr.isnull(), linewidths=0.5)
plt.title('Correlation Matrix of Selected Numerical Variables (Filtered > 0.85)', fontsize=20, fontweight='bold')
plt.show()

# Korelasi pada X_encod
corr_matrix = X_encod.corr()
high_corr_feats = set()
threshold = 0.85

# Menentukan kolom dengan korelasi tinggi
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            high_corr_feats.add(corr_matrix.columns[i])

# Drop kolom yang terlalu berkorelasi
X_filtered = X_encod.drop(columns=high_corr_feats)

# Tampilkan hasil
print("Fitur yang di-drop karena korelasi tinggi (> 0.85):")
print(sorted(high_corr_feats))
print(f"\nTotal kolom yang di-drop: {len(high_corr_feats)}")
print(f"Jumlah fitur sebelum drop: {X_encod.shape[1]}")
print(f"Jumlah fitur setelah drop: {X_filtered.shape[1]}")

"""### **Random Forest Feature Importance**"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Model dan training
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_filtered, y)

# Importance dan seleksi
feature_importance = pd.Series(model.feature_importances_, index=X_filtered.columns)
top_12_feats = feature_importance.sort_values(ascending=False).head(12)
top_12_feats_name = top_12_feats.index.tolist()

# Semua fitur awal
all_features = set(X_filtered.columns)

# Fitur yang dihapus
removed_features = all_features - set(top_12_feats_name)

# Tampilkan hasil
print("Top 12 fitur terpilih berdasarkan Random Forest:")
print(top_12_feats)

print("\nJumlah fitur sebelum seleksi:", len(X_filtered.columns))
print("Jumlah fitur setelah seleksi:", len(top_12_feats_name))
print("Jumlah fitur yang dihapus:", len(removed_features))

print("\nFitur yang dihapus:")
print(sorted(removed_features))

# dibuat untuk skip ulang feature selected random forest hasil target encod buat variabel X_selected berdasarkan 12 fitur teratas
top_12_feats = [
    'last_pymnt_amnt',
    'payment_time',
    'total_pymnt',
    'out_prncp',
    'loan_amnt',
    'total_rec_int',
    'int_rate',
    'issue_month',
    'issue_year',
    'dti',
    'revol_util',
    'revol_bal'
]

X_selected = X_filtered[top_12_feats]

# Cek bentuk akhir dari X_selected
print(f'X_selected shape: {X_selected.shape}')
X_selected.head()

"""Untuk hasil dari metode target encod, di ambil top 12 fitur terlebih dahulu, akan di evaluasi

Fitur last_pymnt_amnt adalah yang paling berpengaruh terhadap prediksi model.

Disusul payment_time, total_pymnt, dan seterusnya.

Fitur seperti installment, funded_amnt, atau hasil encoding lain kemungkinan kontribusinya kecil sehingga tidak dipilih.

## **Feature scalling**
"""

# standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# convert scaled array to dataframe
X_scaled = pd.DataFrame(X_scaled, columns=top_12_feats_name, index=X.index)
X_scaled

"""## **Spliting Data**"""

# split data into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.1, random_state=42, shuffle=True)

print(f'Training set dimension: {X_train.shape}')
print(f'Testing set dimension: {X_test.shape}')

"""## **Handling Imbalanced Data**"""

# Warna kategori
colors = ['#16A34A', '#F97316']  # Hijau untuk Low-Risk (0), Oren untuk High-Risk (1)

# Plot distribusi label target
plt.figure(figsize=(7,5))
sns.countplot(x=y_train, palette=colors)
plt.title("Distribusi Kelas dalam Dataset", fontsize=14, fontweight='bold')
plt.xlabel("Class")
plt.ylabel("Jumlah")
plt.xticks(ticks=[0,1], labels=['Low-Risk = 0 ', 'High-Risk = 1'])
plt.show()

# resampling using SMOTE
smote = SMOTE(random_state=42)
X_tr_resampled, y_tr_resampled = smote.fit_resample(X_train, y_train)

# recheck label proportion
print(pd.Series(y_tr_resampled.value_counts(normalize=True)))

"""Setelah dilakukan SMOTE (Synthetic Minority Over-sampling Technique), proporsi kelas di data training sekarang sudah seimbang:

50% adalah kelas 0 → Low-Risk

50% adalah kelas 1 → High-Risk

# **Function Modelling**

Proses klasifikasi dilakukan dengan menggunakan beberapa pilihan algoritma, seperti Logistic Regression, Random Forest Classifier dan XGBBoost. Hasil evaluasi setiap model kemudian akan dibandingkan pada sebuah tabel dan divisualisasikan untuk mempermudah menentukan model dengan performa terbaik.

## **Function Learning Curve**

learning_curve : membantu memvisualisasikan hubungan antara ukuran data latih dan akurasi, guna mendeteksi potensi overfitting atau underfitting sejak awal.
"""

from sklearn.model_selection import learning_curve, StratifiedKFold
import numpy as np
import matplotlib.pyplot as plt

# Improved learning curve function
def learning_curve_plot(model, X, y, cv=5):
    # Setup Stratified K-Fold
    cv = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)

    # Set seaborn style
    sns.set_style('whitegrid')

    # Generate learning curve
    train_sizes, train_scores, test_scores = learning_curve(
        model, X, y, cv=cv, scoring='accuracy',
        n_jobs=-1, train_sizes=np.linspace(0.3, 1.0, 5)
    )

    # Calculate mean and std
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    # Plot
    plt.figure(figsize=(10, 6))

    # Training curve
    plt.plot(train_sizes, train_mean, 'o-', color='#1f77b4', label='Training Accuracy')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std,
                     alpha=0.2, color='#1f77b4')

    # Validation curve
    plt.plot(train_sizes, test_mean, 'o-', color='#ff7f0e', label='Cross-Validation Accuracy')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std,
                     alpha=0.2, color='#ff7f0e')

    # Enhancements
    plt.title(f'Learning Curve: {model.__class__.__name__}', fontsize=14, fontweight='bold')
    plt.xlabel('Training Set Size', fontsize=12)
    plt.ylabel('Accuracy', fontsize=12)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.legend(fontsize=11)
    plt.grid(True, linestyle='--', alpha=0.6)

    # Add max accuracy annotation
    plt.text(train_sizes[-1], train_mean[-1] + 0.002, f'{train_mean[-1]:.3f}', color='#1f77b4')
    plt.text(train_sizes[-1], test_mean[-1] + 0.002, f'{test_mean[-1]:.3f}', color='#ff7f0e')

    plt.tight_layout()
    plt.show()

"""## **Function Train & test Accuracy**

train_test_acc() membandingkan akurasi data latih dan uji, memberikan gambaran seberapa baik model generalisasi terhadap data baru.
"""

# Function to evaluate and compare training and testing accuracy
def train_test_acc(model, X_train, y_train, X_test, y_test):
    train_acc = model.score(X_train, y_train)
    test_acc = model.score(X_test, y_test)
    diff = train_acc - test_acc

    print(f"Training Accuracy  : {train_acc:.3f}")
    print(f"Testing Accuracy   : {test_acc:.3f}")
    print(f"Generalization Gap : {diff:.3f}")

    if diff > 0.05:
        print("⚠️  Model might be overfitting.")
    elif diff < -0.01:
        print("⚠️  Model might be underfitting.")
    else:
        print("✅  Model shows good generalization.")

"""## **Function Evaluate Model**

evaluate_model : menyajikan metrik evaluasi penting seperti accuracy, precision, recall, F1-score, ROC-AUC, serta classification report dan confusion matrix.
"""

from sklearn.model_selection import cross_val_predict, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
import numpy as np

# Function to display model evaluation: metrics, confusion matrix, and classification report
def evaluate_model(model, X, y, cv=5):
    # Model predictions (cross-validated)
    y_pred = cross_val_predict(model, X, y, cv=cv, n_jobs=-1)
    y_pred_proba = cross_val_predict(model, X, y, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]

    # Confusion matrix & classification report
    cm = confusion_matrix(y, y_pred)
    cr = classification_report(y, y_pred)

    # Metric evaluations
    metrics = {
        'Accuracy': np.round(cross_val_score(model, X, y, cv=cv, scoring='accuracy', n_jobs=-1).mean(), 3),
        'Precision': np.round(cross_val_score(model, X, y, cv=cv, scoring='precision', n_jobs=-1).mean(), 3),
        'Recall': np.round(cross_val_score(model, X, y, cv=cv, scoring='recall', n_jobs=-1).mean(), 3),
        'F1-Score': np.round(cross_val_score(model, X, y, cv=cv, scoring='f1', n_jobs=-1).mean(), 3),
        'ROC-AUC': np.round(roc_auc_score(y, y_pred_proba), 3)
    }

    # Display evaluation
    print(f"\n==== {model.__class__.__name__} Evaluation ====")
    print("\nConfusion Matrix:")
    print(cm)
    print("\nCross-Validated Metrics:")
    for metric, value in metrics.items():
        print(f"{metric}: {value}")
    print("\nClassification Report:")
    print(cr)

    # Return as summary dict
    summary = {
        'Model': model.__class__.__name__,
        'Confusion Matrix': cm,
        **metrics  # unpacking metrics into summary
    }

    return summary

"""## **Function Confusion Matrix**

cm_visualization() mempermudah interpretasi hasil prediksi lewat visualisasi confusion matrix dalam bentuk heatmap.
"""

from sklearn.metrics import confusion_matrix
from matplotlib.colors import LinearSegmentedColormap


def cm_visualization(model, cm, labels):
    # Hitung persentase
    cm_perc = cm / cm.sum()

    # Label isi: TP, FP, FN, TN + jumlah + persentase
    base_labels = np.array([['TP', 'FP'], ['FN', 'TN']])
    labels_annot = np.array([
        f'{label}\n{count} ({perc:.2%})'
        for label, count, perc in zip(base_labels.flatten(), cm.flatten(), cm_perc.flatten())
    ]).reshape(2, 2)

    # Custom colormap
    colors = ['#E8F5E9', '#16A34A', '#FBE9E7', '#F97316']
    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', colors)

    # Plot
    plt.figure(figsize=(6, 6))
    sns.heatmap(
        cm, annot=labels_annot, fmt='', cmap=custom_cmap, cbar=False,
        annot_kws={'size': 13, 'weight': 'bold'}, linewidths=2, linecolor='black',
        xticklabels=['Low-Risk (0)', 'High-Risk (1)'],
        yticklabels=['Low-Risk (0)', 'High-Risk (1)'],
        vmin=0, vmax=cm.max()
    )

    # Styling
    plt.xticks(fontsize=12, weight='bold')
    plt.yticks(fontsize=12, weight='bold')
    plt.xlabel('Predicted', fontsize=14, weight='bold')
    plt.ylabel('Actual', fontsize=14, weight='bold')
    plt.title(f'{model.__class__.__name__} - Confusion Matrix', fontsize=16, weight='bold')

    # Keterangan
    plt.figtext(0.5, -0.05, 'Note: Low-Risk Loan = 0, High-Risk Loan = 1',
                wrap=True, horizontalalignment='center', fontsize=11, style='italic')

    plt.tight_layout()
    plt.show()

"""## **Model Training**"""

# Import semua model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import GradientBoostingClassifier

"""### **1. Logistic Regression**"""

# logistic regression model
lr = LogisticRegression(class_weight='balanced', max_iter=1000, solver='saga', random_state=42)

# learning curve
learning_curve_plot(lr, X_tr_resampled, y_tr_resampled)

# model training
lr.fit(X_tr_resampled, y_tr_resampled)

# model evaluation
lr_result = evaluate_model(lr, X_test, y_test)

# training and testing accuracy gap
train_test_acc(lr, X_tr_resampled, y_tr_resampled, X_test, y_test)

# confusion matrix visualization
cm_visualization(lr, lr_result['Confusion Matrix'], labels=np.unique(y_test))

"""### **2. XG Boost**"""

xgb = XGBClassifier(eval_metric='logloss', random_state=42)

# learning curve
learning_curve_plot(xgb, X_tr_resampled, y_tr_resampled)

# model evaluation
xgb_result = evaluate_model(xgb, X_test, y_test)

# model training
xgb.fit(X_tr_resampled, y_tr_resampled)

# model evaluation
xgb_result = evaluate_model(xgb, X_test, y_test)

# training and testing accuracy gap
train_test_acc(xgb, X_tr_resampled, y_tr_resampled, X_test, y_test)

# confusion matrix visualization
cm_visualization(xgb, xgb_result['Confusion Matrix'], labels=np.unique(y_test))

"""### **3. Random Forest**"""

# random forest classifier model
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# # learning curve
# learning_curve_plot(rf, X_tr_resampled, y_tr_resampled)

# model training
rf.fit(X_tr_resampled, y_tr_resampled)

# model evaluation
rf_result = evaluate_model(rf, X_test, y_test)

# training and testing accuracy gap
train_test_acc(rf, X_tr_resampled, y_tr_resampled, X_test, y_test)

# confusion matrix visualization
cm_visualization(rf, rf_result['Confusion Matrix'], labels=np.unique(y_test))

"""### **4. LightGBM**"""

lgbm = LGBMClassifier(random_state=42)

# # learning curve
# learning_curve_plot(lgbm, X_tr_resampled, y_tr_resampled)

# model training
lgbm.fit(X_tr_resampled, y_tr_resampled)

# model evaluation
lgbm_result = evaluate_model(lgbm, X_test, y_test)

# training and testing accuracy gap
train_test_acc(lgbm, X_tr_resampled, y_tr_resampled, X_test, y_test)

# confusion matrix visualization
cm_visualization(lgbm, lgbm_result['Confusion Matrix'], labels=np.unique(y_test))

"""### **5.Gradient Boosting**"""

gb = GradientBoostingClassifier(random_state=42)

# # learning curve
# learning_curve_plot(gb, X_tr_resampled, y_tr_resampled)

# model training
gb.fit(X_tr_resampled, y_tr_resampled)

# model evaluation
gb_result = evaluate_model(gb, X_test, y_test)

# training and testing accuracy gap
train_test_acc(gb, X_tr_resampled, y_tr_resampled, X_test, y_test)

# confusion matrix visualization
cm_visualization(gb, gb_result['Confusion Matrix'], labels=np.unique(y_test))

"""## **Comparison Model**"""

# # Simpan semua model kamu di list
# models = [
#     ('LogisticRegression', lr),
#     ('XGBClassifier', xgb),
#     ('RandomForestClassifier', rf),
#     ('LGBMClassifier', lgbm),
#     ('GradientBoostingClassifier', gb)
# ]

# # Simpan semua hasil ke dalam list of dicts
# results = []

# for name, model in models:
#     y_pred = model.predict(X_test)
#     y_train_pred = model.predict(X_train)

#     # Skor
#     accuracy = accuracy_score(y_test, y_pred)
#     auc_train = roc_auc_score(y_train, y_train_pred)
#     auc_test = roc_auc_score(y_test, y_pred)

#     recall_train = recall_score(y_train, y_train_pred)
#     recall_test = recall_score(y_test, y_pred)

#     precision_train = precision_score(y_train, y_train_pred)
#     precision_test = precision_score(y_test, y_pred)

#     f1_train = f1_score(y_train, y_train_pred)
#     f1_test = f1_score(y_test, y_pred)

#     crossval_auc = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()

#     # Simpan ke dict
#     results.append({
#         'Model': name,
#         'Accuracy': accuracy,
#         'AUC_train': auc_train,
#         'AUC_test': auc_test,
#         'Recall_train': recall_train,
#         'Recall_test': recall_test,
#         'Precision_train': precision_train,
#         'Precision_test': precision_test,
#         'F1_train': f1_train,
#         'F1_test': f1_test,
#         'CrossVal_AUC': crossval_auc
#     })

# # Ubah ke DataFrame
# results_df = pd.DataFrame(results)

# # Tampilkan hasil
# results_df.sort_values(by='AUC_test', ascending=False).reset_index(drop=True)

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix
)

# Daftar model yang akan dievaluasi
models = [
    ('LogisticRegression', lr),
    ('XGBClassifier', xgb),
    ('RandomForestClassifier', rf),
    ('LGBMClassifier', lgbm),
    ('GradientBoostingClassifier', gb)
]

# Simpan hasil ke dalam list
results = []

for name, model in models:
    # Prediksi dan probabilitas dengan cross-validation
    y_pred = cross_val_predict(model, X_train, y_train, cv=5, n_jobs=-1)
    y_proba = cross_val_predict(model, X_train, y_train, method='predict_proba', cv=5, n_jobs=-1)[:, 1]

    # Hitung metrik evaluasi
    accuracy = accuracy_score(y_train, y_pred)
    precision = precision_score(y_train, y_pred)
    recall = recall_score(y_train, y_pred)
    f1 = f1_score(y_train, y_pred)
    roc_auc = roc_auc_score(y_train, y_proba)

    # Confusion Matrix
    cm = confusion_matrix(y_train, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # Simpan ke dict
    results.append({
        'Model': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc,
        'TP': tp,
        'FP': fp,
        'FN': fn,
        'TN': tn
    })

# Konversi ke DataFrame
results_df = pd.DataFrame(results)

# Kolom metrik yang akan diberikan gradasi warna hijau
metric_cols = [
    'Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'
]

# Normalisasi untuk styling
norm_df = results_df.copy()
for col in metric_cols:
    min_val = norm_df[col].min()
    max_val = norm_df[col].max()
    norm_df[col] = (norm_df[col] - min_val) / (max_val - min_val)

# Fungsi gradasi warna hijau
def green_gradient(val):
    if pd.isna(val):
        return ''
    return f'background-color: rgba(22, 163, 74, {val})'  # hijau Tailwind

# Styling hasil akhir
styled = results_df.style.apply(
    lambda col: [green_gradient(v) if col.name in metric_cols else '' for v in norm_df[col.name]],
    axis=0
).format(precision=3).set_caption("Cross-Validated Performance of Classification Models (5-Fold CV)")

# Tampilkan tabel styled
styled

"""XGBoost dan RandomForest adalah dua model terkuat dengan skor AUC_test = 0.941 dan 0.937, serta F1_test = 0.930 dan 0.927 — artinya model ini seimbang dalam mengidentifikasi kedua kelas (Low-Risk & High-Risk).

Logistic Regression tampil paling ringan dan cepat, tapi performanya masih di bawah model boosting dan random forest (AUC_test = 0.885, F1_test = 0.892).

Semua model tidak menunjukkan overfitting yang parah, karena gap train-test tidak terlalu besar.

Gradient Boosting performa stabil tapi sedikit lebih rendah dibanding XGBoost dan LGBM.

## **Hyperparameter Tuning**

Tujuan utama kita adalah membuat model klasifikasi yang tidak hanya akurat, tapi juga seimbang dalam mendeteksi kredit yang berisiko tinggi (High-Risk) dan tidak terlalu overfit pada data training. Karena itu, kita perlu menyempurnakan model—bukan hanya pakai yang default.

### **1. Tuning XGBoostClassifier**

"XGBoost tampil sebagai salah satu bintang utama pada pemodelan kasus ini"

    Kelebihan:

        F1 Score Test tertinggi (0.930).

        Konsisten antara Train & Test: menandakan tidak overfitting.

        Sudah terbukti kuat untuk dataset tabular seperti data kredit.

    Alasan dipilih untuk tuning:

        Untuk mengoptimalkan lebih lanjut trade-off antara precision dan recall.

        Bisa sangat powerful kalau di-tuning dengan tepat.
"""

from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import joblib
import numpy as np

# 1. Parameter grid untuk RandomizedSearch
param_dist_xgb = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [3, 4, 5, 6, 7],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 1.0],
    'gamma': [0, 1, 3, 5]
}

# 2. Inisialisasi model XGBoost
xgb_model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# 3. Setup RandomizedSearchCV (scoring diganti ke recall)
random_search_xgb = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist_xgb,
    n_iter=30,                   # Hanya 30 kombinasi acak
    scoring='recall',            # Fokus pada recall!
    cv=5,
    verbose=2,
    random_state=42,
    n_jobs=-1
)

# 4. Training
random_search_xgb.fit(X_train, y_train)

# 5. Hasil terbaik
print("Best Parameters:", random_search_xgb.best_params_)
print("Best Recall Score (CV):", random_search_xgb.best_score_)

# 6. Simpan model terbaik
best_xgb_model = random_search_xgb.best_estimator_
joblib.dump(best_xgb_model, 'best_xgb_model_randomcv.pkl')
print("Model saved as 'best_xgb_model_randomcv.pkl'")

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
import joblib

# 1. Parameter grid untuk tuning
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'gamma': [0, 1, 5]
}

# 2. Inisialisasi model XGBoost
xgb_model = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# 3. Setup GridSearchCV
grid_search_xgb = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid_xgb,
    scoring='f1',       # Fokus ke F1 Score (karena kita ingin seimbang antara Precision & Recall)
    cv=5,
    verbose=2,
    n_jobs=-1
)

# 4. Training grid search
grid_search_xgb.fit(X_train, y_train)

# 5. Cetak hasil terbaik
print("Best Parameters:", grid_search_xgb.best_params_)
print("Best F1 Score (CV):", grid_search_xgb.best_score_)

# 6. Ambil model terbaik
best_xgb_model_gridcv = grid_search_xgb.best_estimator_

# 7. Simpan model
joblib.dump(best_xgb_model_gridcv, 'best_xgb_model_gridcv.pkl')
print("Model saved as 'best_xgb_model_gridcv.pkl'")

"""### **2. Tuning RandomForestClassifier**

“Random Forest memberi hasil yang hampir sempurna di data training dan sangat solid di testing.”

    Kelebihan:

        Sangat tinggi di semua metrik, termasuk AUC dan F1.

        Tidak terlalu kompleks dan lebih cepat di-training dibanding XGBoost.

    Alasan dipilih untuk tuning:

        Sedikit tanda overfitting (Recall_train = 1.0), jadi tuning bisa bantu membuat generalisasi lebih baik.

        Model interpretatif dan powerful untuk klasifikasi.
"""

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# import joblib

# # 1. Parameter grid
# param_grid_rf = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'max_features': ['sqrt', 'log2']
# }

# # 2. Inisialisasi model
# rf_model = RandomForestClassifier(random_state=42)

# # 3. Grid search
# grid_search_rf = GridSearchCV(
#     estimator=rf_model,
#     param_grid=param_grid_rf,
#     scoring='f1',      # Bisa diganti dengan 'roc_auc' sesuai objektif
#     cv=5,
#     verbose=2,
#     n_jobs=-1
# )

# # 4. Fit model
# grid_search_rf.fit(X_train, y_train)

# # 5. Cetak hasil terbaik
# print("Best Parameters:", grid_search_rf.best_params_)
# print("Best F1 Score (CV):", grid_search_rf.best_score_)

# # 6. Ambil model terbaik
# best_rf_model = grid_search_rf.best_estimator_

# # 7. Simpan model
# joblib.dump(best_rf_model, 'best_rf_model.pkl')
# print("Model saved as 'best_rf_model.pkl'")

"""### **3. Tuning LGBMClassifier**

“Model paling efisien dan cepat di antara ketiganya.”

    Kelebihan:

        Hampir menyamai performa XGBoost.

        Sangat cepat dalam training dan bisa di-scale untuk dataset besar.

        F1 Test = 0.914 dan CrossVal_AUC = 0.985 → sangat stabil!

    Alasan dipilih untuk tuning:

        Sering outperform XGBoost kalau parameter-nya pas.

        Bisa lebih ringan untuk deployment (computationally cheaper).
"""

# from lightgbm import LGBMClassifier
# from sklearn.model_selection import GridSearchCV
# import joblib

# # 1. Parameter grid
# param_grid_lgb = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [-1, 5, 10],
#     'num_leaves': [31, 50, 100],
#     'learning_rate': [0.01, 0.05, 0.1],
#     'subsample': [0.7, 0.8, 1.0],
#     'colsample_bytree': [0.7, 0.8, 1.0]
# }

# # 2. Inisialisasi model
# lgb_model = LGBMClassifier(random_state=42)

# # 3. Grid search
# grid_search_lgb = GridSearchCV(
#     estimator=lgb_model,
#     param_grid=param_grid_lgb,
#     scoring='f1',      # Bisa ganti ke 'roc_auc' atau 'accuracy' tergantung fokus
#     cv=5,
#     verbose=2,
#     n_jobs=-1
# )

# # 4. Fit model
# grid_search_lgb.fit(X_train, y_train)

# # 5. Cetak hasil terbaik
# print("Best Parameters:", grid_search_lgb.best_params_)
# print("Best F1 Score (CV):", grid_search_lgb.best_score_)

# # 6. Ambil model terbaik
# best_lgb_model = grid_search_lgb.best_estimator_

# # 7. Simpan model
# joblib.dump(best_lgb_model, 'best_lgb_model.pkl')
# print("Model saved as 'best_lgb_model.pkl'")

"""# **Evaluation**

Model terbaik untuk kasus ini adalah menggunakan XGBoost setelah di tuning menggunakan randomized,

Karena model default XGBoost sudah sangat kuat, maka RandomizedSearchCV cukup untuk fine-tuning parameter tanpa membuang banyak waktu dan computational resource.
Dengan hanya 150 kombinasi,sudah berhasil mencapai F1 Score CV sebesar 0.9309, yang memperkuat kestabilan model secara keseluruhan dan mendekati performa maksimalnya.
"""

# prompt: load best_xgb_model_random.pkl

import joblib

# Load the saved model
best_xgb_model_random = joblib.load('best_xgb_model_random.pkl')

# Now you can use the loaded model to make predictions
# Example:
# y_pred = best_xgb_model.predict(X_new)

# model evaluation
best_xgb_model_random= evaluate_model(best_xgb_model_random, X_test, y_test)

# 1. Predict menggunakan model hasil tuning
y_pred_best = best_xgb_model.predict(X_test)

# 2. Buat confusion matrix
cm_best = confusion_matrix(y_test, y_pred_best)

# 3. Simpan ke dalam dictionary (opsional, jika kamu simpan ke result dictionary)
best_xgb_model_result = {
    'Model': 'XGBoost Tuned',
    'Confusion Matrix': cm_best,
    'F1 Score': f1_score(y_test, y_pred_best),
    'Accuracy': accuracy_score(y_test, y_pred_best),
    'Precision': precision_score(y_test, y_pred_best),
    'Recall': recall_score(y_test, y_pred_best),
    'ROC AUC': roc_auc_score(y_test, y_pred_best)
}

# 4. Visualisasikan
cm_visualization(best_xgb_model, best_xgb_model_result['Confusion Matrix'], labels=np.unique(y_test))

# 1. Ambil feature importances dari model yang sudah dituning
feature_importances = best_xgb_model.feature_importances_

# 2. Buat DataFrame
importance_df = pd.DataFrame({
    'Feature': X_train.columns,     # pastikan kolom fitur sesuai
    'Importance': feature_importances
})

# 3. Urutkan dari yang paling penting
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# 4. Tampilkan 12 fitur teratas
print("Top 12 Most Important Features:")
print(importance_df.head(12))

# Ambil semua 10 fitur
top_10_features = importance_df.head(10)

# Bagi menjadi dua grup: 7 teratas dan 3 sisanya
top_7 = top_10_features.head(7)
next_3 = top_10_features.iloc[7:]

# Buat figure
plt.figure(figsize=(10, 6))

# Barplot untuk 7 teratas
sns.barplot(x='Importance', y='Feature', data=top_7, color='skyblue', label='Top 7')

# Barplot untuk 3 sisanya
sns.barplot(x='Importance', y='Feature', data=next_3, color='lightcoral', label='Next 3')

plt.title('Feature Importance (Top 10)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.legend()
plt.tight_layout()
plt.show()

"""# **Metrics Impact Simulation**"""

# Confusion matrix results
tp = 41065
fp = 19
fn = 691
tn = 4839

# Total before
total_clients = df.shape[0]
total_clients_X = X_scaled.shape[0]
total_low_risk_before = df["loan_status"].value_counts()[0]  # Low-Risk Loan
total_high_risk_before = df["loan_status"].value_counts()[1]  # High-Risk Loan
total_loan = df["funded_amnt"].sum()
avg_loan = total_loan / df.shape[0]

# High-Risk Rate before & after
high_risk_rate_before = total_high_risk_before / total_clients
high_risk_rate_after = (fp + fn) / total_clients_X

# Change in High-Risk Rate
change_high_risk_rate = high_risk_rate_after - high_risk_rate_before

# Total high-risk loans after
total_high_risk_after = high_risk_rate_after * total_clients

# Decrease in high-risk loans
dec_high_risk_loans = total_high_risk_after - total_high_risk_before

# Total revenue, high-risk loss, and net revenue before
tr_before = total_low_risk_before * avg_loan
total_hr_loss_before = total_high_risk_before * avg_loan
net_rev_before = tr_before - total_hr_loss_before

# Total low-risk clients after
total_low_risk_after = total_clients - total_high_risk_after

# Total revenue, high-risk loss, and net revenue after
tr_after = total_low_risk_after * avg_loan
total_hr_loss_after = total_high_risk_after * avg_loan
net_rev_after = tr_after - total_hr_loss_after

# Percentage calculations
perc_dec_high_risk_loans = ((total_high_risk_before - total_high_risk_after) / total_high_risk_before) * 100
perc_inc_total_revenue = ((tr_after - tr_before) / tr_before) * 100
perc_dec_high_risk_loss = ((total_hr_loss_before - total_hr_loss_after) / total_hr_loss_before) * 100
perc_inc_net_revenue = ((net_rev_after - net_rev_before) / net_rev_before) * 100

# Print the results
print(f'Total High-Risk Loans before: {"{:,}".format(total_high_risk_before)}')
print(f'Total High-Risk Loans after: {"{:,}".format(round(total_high_risk_after))}')
print(f'Decrease in High-Risk Loans: {"{:,}".format(round(dec_high_risk_loans))} ({round(perc_dec_high_risk_loans, 2)}%)\n')

print(f'High-Risk Rate before: {round(high_risk_rate_before * 100, 2)}%')
print(f'High-Risk Rate after: {round(high_risk_rate_after * 100, 2)}%')
print(f'Change in High-Risk Rate: {round(change_high_risk_rate * 100, 2)}%\n')

print(f'Total Revenue before: {"{:,}".format(round(tr_before))}')
print(f'Total Revenue after: {"{:,}".format(round(tr_after))}')
print(f'Increase in Total Revenue: {"{:,}".format(round(tr_after - tr_before))} ({round(perc_inc_total_revenue, 2)}%)\n')

print(f'Total High-Risk Loss before: {"{:,}".format(round(total_hr_loss_before))}')
print(f'Total High-Risk Loss after: {"{:,}".format(round(total_hr_loss_after))}')
print(f'Decrease in High-Risk Loss: {"{:,}".format(round(total_hr_loss_before - total_hr_loss_after))} ({round(perc_dec_high_risk_loss, 2)}%)\n')

print(f'Net Revenue before: {"{:,}".format(round(net_rev_before))}')
print(f'Net Revenue after: {"{:,}".format(round(net_rev_after))}')
print(f'Increase in Net Revenue: {"{:,}".format(round(net_rev_after - net_rev_before))} ({round(perc_inc_net_revenue, 2)}%)')

"""**Business Metrics**

🔹 High-Risk Rate – Default Rate (DR)

Rasio nasabah gagal bayar terhadap total nasabah.

Sebelum: 11.86%

Sesudah: 0.15%

Perubahan: ↓11.71%

🔹 Total High-Risk Loans – Exposure at Default (EAD)

Jumlah pinjaman dari nasabah yang berpotensi gagal bayar.

Sebelum: 55,304

Sesudah: 710

Perubahan: ↓54,594 (↓98.72%)

🔹 Total Revenue – Expected Interest Income

Pendapatan kotor yang dihasilkan dari pinjaman aktif.

Sebelum: Rp 5,872,683,387

Sesudah: Rp 6,653,075,750

Perubahan: ↑Rp 780,392,363 (+13.29%)

🔹 Total High-Risk Loss – Expected Loss (EL)
Potensi kerugian akibat gagal bayar.

Sebelum: 790,541,438

Sesudah: Rp 10,149,075

Perubahan: ↓Rp 780,392,363 (↓98.72%)

🔹 Net Revenue – Net Interest Income (NII)

Pendapatan bersih setelah dikurangi kerugian dari kredit macet.

Sebelum: Rp 5,082,141,950

Sesudah: Rp 6,642,926,676

Perubahan: ↑Rp 1,560,784,726 (+30.71%)

# **Business Recomendation**

**Strategi Bisnis Berbasis Machine Learning untuk Optimasi Risiko Kredit**

Dengan bantuan model XGBoost yang sudah dituning, sistem prediksi risiko kredit kita berhasil menurunkan high-risk loan sebesar 98.72%, meningkatkan total revenue hingga +13.29%, dan mendorong kenaikan net revenue sebesar +30.71%.

Bagaimana kita bisa mempertahankan dan bahkan mengembangkan impact ini secara berkelanjutan?

Berikut 3 Strategi Inti + Insight & Aksi Bisnis Nyata:

### **1. Fast-Track Approval & Loyal Nasabah = Revenue Booster**

**Insight**: Fitur last_pymnt_amnt, payment_time, dan total_pymnt jadi sinyal paling kuat nasabah dengan riwayat pembayaran sehat. Mereka adalah segmen low-risk yang mendongkrak revenue tanpa menambah risiko.

**Strategi Kekinian:**

*  Implementasi fast-track approval untuk nasabah dengan skor prediksi rendah risiko sehingga proses lebih cepat dan efisiensi tinggi.

*  Tambahkan loyalty rewards seperti cashback, bunga lebih rendah, dan akses eksklusif bagi nasabah disiplin.

* Gunakan gamifikasi (badge pembayaran, leaderboard loyalitas) untuk menjaga engagement tinggi.

**Dampak Real:**

* Meningkatkan konversi pinjaman → kontribusi pada kenaikan revenue +13.29%

* Memperkuat retensi pelanggan sehat → long-term lifetime value

* Hemat waktu dan biaya operasional verifikasi manual

### **2. Smart Monitoring untuk Nasabah Aktif dengan Risiko Tersembunyi**

**Insight**: Fitur out_prncp dan loan_amnt jadi early warning signal. Nasabah dengan pokok tersisa tinggi tapi pembayaran terakhir kecil = potensi gagal bayar tinggi.

**Strategi Kekinian:**

* Buat dashboard monitoring aktif sehingga akun dengan outstanding tinggi langsung ditandai.

* Kirim notifikasi atau reminder otomatis saat ada penurunan pembayaran, sehingga intervensi lebih awal.

* Jalankan AI-based offer: tawarkan restrukturisasi cicilan untuk mencegah default.

**Dampak Real:**

* Penurunan drastis high-risk loans dari 55.304 menjadi 710 (-98.72%)

* High-risk loss turun dari 790M menjadi 10M (-98.72%)

* Turunkan beban operasional dan risiko keuangan

### **3. Dynamic Produk Berdasarkan Pola Pembayaran & Musiman**

**Insight:** Fitur issue_month dan total_rec_int mengindikasikan bahwa waktu pengajuan & struktur bunga ikut mempengaruhi risiko. Ada bulan-bulan rawan yang perlu strategi khusus.

**Strategi Kekinian:**

* Gunakan dynamic credit scoring dengan update real-time berdasarkan payment_time dan bulan pengajuan.

* Hindari promo agresif di bulan-bulan rawan default, ganti dengan promo musiman di periode aman (Q1 misalnya).

* Buat sistem penawaran produk adaptif, sehingga tenor & bunga disesuaikan otomatis dengan histori peminjam.

**Dampak Real:**

* Kurangi potensi gagal bayar karena promo di waktu yang salah

* Optimalkan penetrasi produk pada momen yang terbukti lebih aman

* Naikkan net revenue menjadi +30.71%

Model Machine Learning + Strategi Agile = Transformasi Bisnis Nyata
Kita nggak cuma prediksi siapa yang gagal bayar, tapi juga bisa bentuk perilaku nasabah, desain produk lebih adaptif, dan maksimalkan return dengan minim risiko.

**Turunkan potensi gagal bayar**

**Naikkan efisiensi & kecepatan operasional**

**Dorong revenue & retensi dengan pendekatan yang customer-centric**
"""